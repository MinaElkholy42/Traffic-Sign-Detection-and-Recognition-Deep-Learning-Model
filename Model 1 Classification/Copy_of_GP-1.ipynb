{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GP",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GaPq77uwAK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rexPfVNhLmfT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCrkO-i7cEk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scikit-image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBZ_-UP_edhg",
        "colab_type": "code",
        "outputId": "8710ab10-785c-4b95-e6f4-5d619ebd4d66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import skimage as sk\n",
        "from random import shuffle\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras import backend as k\n",
        "from skimage import morphology, filters\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "IMG_SIZE = 32\n",
        "# how many samples to calculte the error\n",
        "BATCH_SIZE = 32\n",
        "TRAIN_DIR = \"/content/drive/My Drive/Data Set Traffice Signs detection/Train\"\n",
        "TEST_DIR = \"/content/drive/My Drive/Data Set Traffice Signs detection/Test\"\n",
        "classes = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\n",
        "           \"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\n",
        "           \"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\n",
        "           \"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\n",
        "           \"24\",\"25\",\"26\",\"27\",\"28\",\"29\",\n",
        "           \"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\n",
        "           \"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\"\n",
        "           ]\n",
        "def histogram_equalize(image):\n",
        "    kernel =  sk.morphology.disk(30)\n",
        "    img_local = sk.filters.rank.equalize(image, selem=kernel)\n",
        "    return img_local\n",
        "def create_label(folder_name):\n",
        "    \"\"\" Create an one-hot encoded vector from folder name \"\"\"\n",
        "    index = classes.index(folder_name)\n",
        "    return np.array([1 if i == index else 0 for i in range(43)])\n",
        "\n",
        "def create_train_data():\n",
        "    training_data = []\n",
        "    for folder in tqdm(classes):\n",
        "        folder_path = TRAIN_DIR + \"/\" + folder\n",
        "        for train_img in os.listdir(folder_path):\n",
        "            img_path = os.path.join(folder_path, train_img)\n",
        "            # print(img_path)\n",
        "            train_img_data = cv2.imread(img_path, 0)\n",
        "            train_img_data = histogram_equalize(train_img_data)\n",
        "            train_img_data = cv2.resize(train_img_data, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
        "            training_data.append([np.array(train_img_data), create_label(folder)])\n",
        "    shuffle(training_data)\n",
        "    np.save('train_data.npy', training_data)\n",
        "    return training_data\n",
        "\n",
        "def create_test_data():\n",
        "    testing_data = []\n",
        "    for test_img in tqdm(os.listdir(TEST_DIR)):\n",
        "        img_path = os.path.join(TEST_DIR, test_img)\n",
        "        if test_img == \"GT-final_test.csv\":\n",
        "          continue\n",
        "        # print(img_path)\n",
        "        test_img_data = cv2.imread(img_path, 0)\n",
        "        test_img_data = histogram_equalize(test_img_data)\n",
        "        test_img_data = cv2.resize(test_img_data, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
        "        testing_data.append([np.array(test_img_data), test_img])\n",
        "    np.save('test_data.npy', testing_data)\n",
        "    return testing_data\n",
        "train_data_npy = '/content/drive/My Drive/Data Set Traffice Signs detection/train_data.npy'\n",
        "test_data_npy = '/content/drive/My Drive/Data Set Traffice Signs detection/test_data.npy'\n",
        "if os.path.exists(train_data_npy):\n",
        "    train_data_from_file = np.load(train_data_npy, allow_pickle=True)\n",
        "    print(\"Train Data Loaded Successfully\")\n",
        "    print('--------------------------------')\n",
        "else:\n",
        "    train_data_from_file = create_train_data()\n",
        "\n",
        "if os.path.exists(test_data_npy):\n",
        "    test_data_from_file = np.load(test_data_npy, allow_pickle=True)\n",
        "    print(\"Teset Data Loaded Successfully\")\n",
        "    print('--------------------------------')\n",
        "else:\n",
        "    test_data_from_file = create_test_data()\n",
        "\n",
        "train = train_data_from_file\n",
        "test = test_data_from_file\n",
        "\n",
        "X_train = np.array([i[0] for i in train]).reshape((-1, IMG_SIZE, IMG_SIZE, 1))\n",
        "y_train = [i[1] for i in train]\n",
        "y_train = np.asarray(y_train)\n",
        "\n",
        "# X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = 0.2)\n",
        "\n",
        "X_test = np.array([i[0] for i in test]).reshape((-1, IMG_SIZE, IMG_SIZE, 1))\n",
        "y_test = [i[1] for i in test]\n",
        "y_test = np.asarray(y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Model\n",
        "k.clear_session()\n",
        "model = Sequential()\n",
        "model.add(Conv2D(input_shape=(IMG_SIZE,IMG_SIZE,1),filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=32,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\"))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=64, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n",
        "model.add(MaxPool2D(pool_size=(2,2),strides=(2,2), padding=\"VALID\"))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=128,activation=\"relu\"))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=44, activation=\"softmax\"))\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer=tf.keras.optimizers.Adam(lr=0.001))\n",
        "\n",
        "model.fit(x=X_train, y=y_train, epochs=10, validation_split=0.11, batch_size=64)\n",
        "# testing\n",
        "fields = [\"Path\", \"ClassId\"]\n",
        "data = pd.read_csv(\"/content/drive/My Drive/Data Set Traffice Signs detection/Test.csv\",  usecols=fields)\n",
        "test_original = data.to_numpy()\n",
        "row , col = test_original.shape\n",
        "accepted_prediction = 0\n",
        "for img_counter in range(len(X_test)):\n",
        "  test_img_name = 'Test/' + y_test[img_counter]\n",
        "  x_test_4d= np.expand_dims(X_test[img_counter], axis=0)\n",
        "  prediction = model.predict(x_test_4d)\n",
        "  for i in range(row):\n",
        "    if test_original[i][1] == test_img_name:\n",
        "      class_value = test_original[i][0]\n",
        "      break\n",
        "  if(np.argmax(prediction) == class_value):\n",
        "    accepted_prediction += 1\n",
        "  # print('img {} label {} predicter {}'.format(test_img_name, class_value, np.argmax(prediction)))\n",
        "  # print(accepted_prediction)\n",
        "print(\"final acc is {}\".format((accepted_prediction/row) * 100))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Data Loaded Successfully\n",
            "--------------------------------\n",
            "Teset Data Loaded Successfully\n",
            "--------------------------------\n",
            "Train on 34902 samples, validate on 4314 samples\n",
            "Epoch 1/10\n",
            "34902/34902 [==============================] - 8s 240us/sample - loss: 2.0533 - accuracy: 0.4328 - val_loss: 2.2303 - val_accuracy: 0.4223\n",
            "Epoch 2/10\n",
            "34902/34902 [==============================] - 5s 131us/sample - loss: 0.5459 - accuracy: 0.8270 - val_loss: 0.1075 - val_accuracy: 0.9685\n",
            "Epoch 3/10\n",
            "34902/34902 [==============================] - 5s 132us/sample - loss: 0.2813 - accuracy: 0.9110 - val_loss: 0.1042 - val_accuracy: 0.9641\n",
            "Epoch 4/10\n",
            "34902/34902 [==============================] - 5s 135us/sample - loss: 0.2055 - accuracy: 0.9367 - val_loss: 0.0732 - val_accuracy: 0.9773\n",
            "Epoch 5/10\n",
            "34902/34902 [==============================] - 5s 138us/sample - loss: 0.1633 - accuracy: 0.9496 - val_loss: 0.0371 - val_accuracy: 0.9893\n",
            "Epoch 6/10\n",
            "34902/34902 [==============================] - 5s 136us/sample - loss: 0.1348 - accuracy: 0.9572 - val_loss: 0.0525 - val_accuracy: 0.9831\n",
            "Epoch 7/10\n",
            "34902/34902 [==============================] - 5s 132us/sample - loss: 0.1185 - accuracy: 0.9628 - val_loss: 0.0357 - val_accuracy: 0.9891\n",
            "Epoch 8/10\n",
            "34902/34902 [==============================] - 5s 133us/sample - loss: 0.1067 - accuracy: 0.9668 - val_loss: 0.0253 - val_accuracy: 0.9914\n",
            "Epoch 9/10\n",
            "34902/34902 [==============================] - 5s 133us/sample - loss: 0.1044 - accuracy: 0.9683 - val_loss: 0.0217 - val_accuracy: 0.9933\n",
            "Epoch 10/10\n",
            "34902/34902 [==============================] - 5s 132us/sample - loss: 0.0895 - accuracy: 0.9727 - val_loss: 0.0200 - val_accuracy: 0.9933\n",
            "final acc is 96.8646080760095\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lleapZrqCA2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}